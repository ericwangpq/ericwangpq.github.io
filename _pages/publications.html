---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<h2>Research Interest</h2>
<p>I'm interested in computer vision, deep learning, large language models, and embodied AI.</p>

<h2>Publications</h2>

<div class="publication">
  <img src="../images/MSRA.png" alt="Project image" class="publication-image">
  <div class="publication-content">
    <h3>Transferring Foundation Models for Generalizable Robotic Manipulation</h3>
    <p>Jiange Yang, Wenhui Tan, Chuhao Jin, <strong>Keling Yao</strong>, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang</p>
    <p><em>arXiv</em>, 2024</p>
    <p>
      <a href="https://arxiv.org/abs/2306.05716">arXiv</a> |
      <a href="https://www.youtube.com/watch?v=1m9wNzfp_4E&t=1s">Video</a>
    </p>
    <p>We propose a novel paradigm that effectively leverages language-reasoning segmentation mask generated by internet-scale foundation models, to condition robot manipulation tasks.</p>
  </div>
</div>

<div class="publication">
  <img src="../images/dttd2.png" alt="Project image" class="publication-image">
  <div class="publication-content">
    <h3>Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset</h3>
    <p>Zixun Huang*, <strong>Keling Yao*</strong>, Seth Z. Zhao*, Chuanyu Pan*, Tianjian Xu, Weiyu Feng, Allen Y. Yang</p>
    <p><em>arXiv</em>, 2024</p>
    <p>
      <a href="https://github.com/augcog/DTTD2">project page</a> |
      <a href="https://arxiv.org/abs/2309.13570">arXiv</a> |
      <a href="https://youtu.be/QhYWyoPTmOk">Video</a>
    </p>
    <p>We propose a transformer-based 6DoF pose estimator designed to achieve state-of-the-art accuracy under real-world noisy data. To systematically validate the new solution's performance against the prior art, we also introduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2).</p>
  </div>
</div>