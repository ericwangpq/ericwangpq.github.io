---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

<h2>Research Interest</h2>
<p>I'm interested in computer vision, deep learning, large language models, and embodied AI.</p>

<table style="width:100%; margin:auto;">
  <tr style="background-color:#ffffff;">
    <td style="padding:20px; width:160px;">
      <img src="../images/MSRA.png" width="160">
    </td>
    <td style="padding:20px;">
      <strong>Transferring Foundation Models for Generalizable Robotic Manipulation</strong>
      <br>
      Jiange Yang, Wenhui Tan, Chuhao Jin, <strong>Keling Yao</strong>, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang
      <br>
      <em>arXiv</em>, 2024
      <br>
      <a href="https://arxiv.org/abs/2306.05716">arXiv</a> /
      <a href="https://www.youtube.com/watch?v=1m9wNzfp_4E&t=1s">Video</a>
      <p>
        We propose a novel paradigm that effectively leverages language-reasoning segmentation mask generated by internet-scale foundation models, to condition robot manipulation tasks.
      </p>
    </td>
  </tr>
  <tr style="background-color:#ffffff;">
    <td style="padding:20px; width:160px;">
      <img src="../images/dttd2.png" width="160">
    </td>
    <td style="padding:20px;">
      <strong>Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset</strong>
      <br>
      Zixun Huang*, <strong>Keling Yao*</strong>, Seth Z. Zhao*, Chuanyu Pan*, Tianjian Xu, Weiyu Feng, Allen Y. Yang
      <br>
      <em>arXiv</em>, 2024
      <br>
      <a href="https://github.com/augcog/DTTD2">project page</a> /
      <a href="https://arxiv.org/abs/2309.13570">arXiv</a> /
      <a href="https://youtu.be/QhYWyoPTmOk">Video</a>
      <p>
        We propose a transformer-based 6DoF pose estimator designed to achieve state-of-the-art accuracy under real-world noisy data. To systematically validate the new solution's performance against the prior art, we also introduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2).
      </p>
    </td>
  </tr>
</table>