---
layout: archive
title: ""
permalink: /publications/
author_profile: true
---

<h2>Research Interest</h2>
<p>I'm interested in computer vision, deep learning, large language models, and embodied AI.</p>

<!-- Project entries -->
<div class="publication-entries">
  <table style="width:100%; border-collapse: separate; border-spacing: 0 20px; background-color: transparent; border: none;">
    <tr style="background-color: transparent;">
      <td style="width:30%; vertical-align: top; padding-right: 20px; border: none;">
        <img src="{{ base_path }}/images/MSRA.png" alt="" style="width:100%; max-width:200px;">
      </td>
      <td style="width:70%; vertical-align: top; border: none; font-size: 1.1em;">
        <h3 style="font-size: 1.3em;">Transferring Foundation Models for Generalizable Robotic Manipulation</h3>
        <p>Jiange Yang, Wenhui Tan, Chuhao Jin, <strong>Keling Yao</strong>, Bei Liu, Jianlong Fu, Ruihua Song, Gangshan Wu, Limin Wang</p>
        <p><em>arXiv</em>, 2024</p>
        <p><a href="https://arxiv.org/abs/2306.05716">arXiv</a> / <a href="https://www.youtube.com/watch?v=1m9wNzfp_4E&t=1s">Video</a></p>
        <p>We propose a novel paradigm that effectively leverages language-reasoning segmentation mask generated by internet-scale foundation models, to condition robot manipulation tasks.</p>
      </td>
    </tr>
    <tr style="background-color: transparent;">
      <td style="width:30%; vertical-align: top; padding-right: 20px; border: none;">
        <img src="{{ base_path }}/images/dttd2.png" alt="" style="width:100%; max-width:200px;">
      </td>
      <td style="width:70%; vertical-align: top; border: none; font-size: 1.1em;">
        <h3 style="font-size: 1.3em;">Robust Digital-Twin Localization via An RGBD-based Transformer Network and A Comprehensive Evaluation on a Mobile Dataset</h3>
        <p>Zixun Huang*, <strong>Keling Yao*</strong>, Seth Z. Zhao*, Chuanyu Pan*, Tianjian Xu, Weiyu Feng, Allen Y. Yang</p>
        <p><em>arXiv</em>, 2024</p>
        <p><a href="https://github.com/augcog/DTTD2">project page</a> / <a href="https://arxiv.org/abs/2309.13570">arXiv</a> / <a href="https://youtu.be/QhYWyoPTmOk">Video</a></p>
        <p>We propose a transformer-based 6DoF pose estimator designed to achieve state-of-the-art accuracy under real-world noisy data. To systematically validate the new solution's performance against the prior art, we also introduce a novel RGBD dataset called Digital Twin Tracking Dataset v2 (DTTD2).</p>
      </td>
    </tr>
  </table>
</div>